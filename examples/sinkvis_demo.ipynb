{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SinkVis Demo: Attention Sinks & KV Cache Eviction\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Loading GPT-2 and capturing attention patterns\n",
        "2. Visualizing attention sinks with SinkVis\n",
        "3. Comparing eviction policies\n",
        "4. **Needle in a Hayst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install sinkvis package from py-package directory\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import importlib\n",
        "import subprocess\n",
        "\n",
        "package_path = Path.cwd().parent / \"sinkvis\"\n",
        "print(f\"Installing sinkvis from: {package_path}\")\n",
        "\n",
        "# Use pip install -e for editable installation\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(package_path), \"--quiet\"],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "if result.returncode != 0:\n",
        "    print(\"Installation output:\", result.stdout)\n",
        "    if result.stderr:\n",
        "        print(\"Errors:\", result.stderr)\n",
        "    raise RuntimeError(f\"Installation failed with return code {result.returncode}\")\n",
        "\n",
        "# Clear any cached imports of sinkvis\n",
        "if 'sinkvis' in sys.modules:\n",
        "    del sys.modules['sinkvis']\n",
        "# Also clear any submodules\n",
        "modules_to_remove = [m for m in sys.modules.keys() if m.startswith('sinkvis')]\n",
        "for m in modules_to_remove:\n",
        "    del sys.modules[m]\n",
        "\n",
        "# Force reload the import system and import sinkvis\n",
        "importlib.invalidate_caches()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sinkvis\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "from sinkvis import SinkVis\n",
        "from sinkvis.models import EvictionPolicy, SimulationConfig\n",
        "from sinkvis.eviction import run_simulation\n",
        "\n",
        "print(\"âœ“ All imports successful\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Load GPT-2 Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GPT-2 model and tokenizer\n",
        "print(\"Loading GPT-2...\")\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Set padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"âœ“ Loaded {model_name}\")\n",
        "print(f\"  - Layers: {model.config.n_layer}\")\n",
        "print(f\"  - Attention heads: {model.config.n_head}\")\n",
        "print(f\"  - Hidden size: {model.config.n_embd}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Capture Attention with SinkVis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompt\n",
        "prompt = \"The capital of France is Paris. The Eiffel Tower is a famous landmark.\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "tokens = [tokenizer.decode([tok]) for tok in input_ids[0]]\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
        "\n",
        "# Use SinkVis context manager to capture attention\n",
        "with SinkVis(model, tokenizer) as sv:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "    \n",
        "    # Get attention data\n",
        "    attention_data = sv.get_attention_data(layer=-1, head=0)\n",
        "    \n",
        "    if attention_data is not None:\n",
        "        print(f\"âœ“ Captured attention matrix: {len(attention_data)}x{len(attention_data[0])}\")\n",
        "    else:\n",
        "        print(\"âš  No attention captured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Visualize Attention Sinks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize attention heatmap\n",
        "if attention_data is not None:\n",
        "    attention_matrix = np.array(attention_data)\n",
        "    \n",
        "    # Create heatmap\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    im = ax.imshow(attention_matrix, cmap='viridis', aspect='auto')\n",
        "    \n",
        "    # Set ticks\n",
        "    ax.set_xticks(range(len(tokens)))\n",
        "    ax.set_yticks(range(len(tokens)))\n",
        "    ax.set_xticklabels(tokens, rotation=90, ha='right', fontsize=8)\n",
        "    ax.set_yticklabels(tokens, fontsize=8)\n",
        "    \n",
        "    # Labels\n",
        "    ax.set_xlabel('Key Position', fontsize=12)\n",
        "    ax.set_ylabel('Query Position', fontsize=12)\n",
        "    ax.set_title('GPT-2 Attention Pattern (Last Layer, Head 0)', fontsize=14, pad=20)\n",
        "    \n",
        "    # Colorbar\n",
        "    cbar = plt.colorbar(im, ax=ax)\n",
        "    cbar.set_label('Attention Weight', rotation=270, labelpad=20)\n",
        "    \n",
        "    # Identify sinks (high average attention)\n",
        "    avg_attention = attention_matrix.mean(axis=0)\n",
        "    sink_threshold = 0.1\n",
        "    sink_indices = np.where(avg_attention > sink_threshold)[0]\n",
        "    \n",
        "    # Highlight sinks with red boxes\n",
        "    for idx in sink_indices:\n",
        "        ax.add_patch(plt.Rectangle((idx-0.5, -0.5), 1, len(tokens), \n",
        "                                   fill=False, edgecolor='red', linewidth=2))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nðŸ”´ Attention Sinks detected at positions: {sink_indices.tolist()}\")\n",
        "    print(f\"Sink tokens: {[tokens[i] for i in sink_indices]}\")\n",
        "    print(f\"\\nAverage attention per position:\")\n",
        "    for i, (token, attn) in enumerate(zip(tokens, avg_attention)):\n",
        "        marker = \"ðŸ”´\" if i in sink_indices else \"  \"\n",
        "        print(f\"{marker} {i:2d}. {token:15s} {attn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Needle in a Haystack - Entity Preservation Test\n",
        "\n",
        "This test checks if critical entities (like \"Alice\") survive cache eviction under different policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define prompt with specific entity\n",
        "needle_prompt = (\n",
        "    \"Alice went to the market and bought fresh vegetables. \"\n",
        "    \"The weather was nice and sunny. Many people were shopping. \"\n",
        "    \"She met her friend Bob near the fruit stand. \"\n",
        "    \"They discussed their weekend plans and decided to meet again.\"\n",
        ")\n",
        "\n",
        "# Entity to track\n",
        "entity = \"Alice\"\n",
        "\n",
        "# Tokenize and find entity position\n",
        "inputs = tokenizer(needle_prompt, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "tokens = [tokenizer.decode([tok]) for tok in input_ids[0]]\n",
        "\n",
        "# Find entity token index\n",
        "entity_token_idx = None\n",
        "entity_token = None\n",
        "for i, token in enumerate(tokens):\n",
        "    if entity.lower() in token.lower():\n",
        "        entity_token_idx = i\n",
        "        entity_token = token\n",
        "        break\n",
        "\n",
        "print(f\"Prompt: {needle_prompt}\")\n",
        "print(f\"\\nTotal tokens: {len(tokens)}\")\n",
        "print(f\"Target entity: '{entity}'\")\n",
        "print(f\"Entity token: '{entity_token}' at position {entity_token_idx}\")\n",
        "print(f\"\\nAll tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Capture attention for the needle prompt\n",
        "with SinkVis(model, tokenizer) as sv:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "    \n",
        "    # Get attention matrix\n",
        "    attention_data = sv.get_attention_data(layer=-1, head=0)\n",
        "    attention_matrix = np.array(attention_data)\n",
        "    \n",
        "print(f\"âœ“ Captured attention matrix: {attention_matrix.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define cache budget (smaller than sequence length to force eviction)\n",
        "cache_budget = 20  # Keep only 20 tokens out of ~40\n",
        "sink_count = 4\n",
        "\n",
        "# Policies to compare\n",
        "policies = [\n",
        "    (\"Full\", EvictionPolicy.FULL, len(tokens)),\n",
        "    (\"Sliding Window\", EvictionPolicy.SLIDING_WINDOW, cache_budget),\n",
        "    (\"StreamingLLM\", EvictionPolicy.STREAMING_LLM, cache_budget),\n",
        "    (\"H2O\", EvictionPolicy.H2O, cache_budget),\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Running simulations with cache budget: {cache_budget} tokens\")\n",
        "print(f\"Sequence length: {len(tokens)} tokens\")\n",
        "print(f\"Sink count: {sink_count}\")\n",
        "print(f\"{'='*70}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sinkvis.simulation import (\n",
        "    simulate_lru,\n",
        "    simulate_sliding_window,\n",
        "    simulate_streaming_llm,\n",
        "    simulate_h2o,\n",
        ")\n",
        "\n",
        "# Run simulations for each policy\n",
        "for policy_name, policy_type, budget in policies:\n",
        "    print(f\"\\n{'â”€'*70}\")\n",
        "    print(f\"Policy: {policy_name} (budget: {budget})\")\n",
        "    print(f\"{'â”€'*70}\")\n",
        "    \n",
        "    # Generate keep mask based on policy\n",
        "    if policy_name == \"Full\":\n",
        "        mask = np.ones(len(tokens), dtype=bool)\n",
        "    elif policy_name == \"Sliding Window\":\n",
        "        mask = simulate_sliding_window(len(tokens), budget)\n",
        "    elif policy_name == \"StreamingLLM\":\n",
        "        mask = simulate_streaming_llm(len(tokens), budget, sink_count)\n",
        "    elif policy_name == \"H2O\":\n",
        "        mask, _ = simulate_h2o(attention_matrix, budget, sink_count)\n",
        "    elif policy_name == \"LRU\":\n",
        "        mask, _ = simulate_lru(attention_matrix, budget)\n",
        "    \n",
        "    # Calculate VRAM usage (simplified estimation)\n",
        "    # Assume each token's KV cache takes ~2KB (key + value vectors)\n",
        "    bytes_per_token = 2048  # 2KB\n",
        "    num_layers = model.config.n_layer\n",
        "    kept_tokens = mask.sum()\n",
        "    vram_mb = (kept_tokens * bytes_per_token * num_layers) / (1024 * 1024)\n",
        "    \n",
        "    # Check if entity token is preserved\n",
        "    entity_preserved = mask[entity_token_idx] if entity_token_idx is not None else False\n",
        "    \n",
        "    # Store results\n",
        "    results.append({\n",
        "        'policy': policy_name,\n",
        "        'vram_mb': vram_mb,\n",
        "        'kept_tokens': kept_tokens,\n",
        "        'entity_preserved': entity_preserved,\n",
        "        'mask': mask,\n",
        "    })\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"Tokens kept: {kept_tokens}/{len(tokens)}\")\n",
        "    print(f\"VRAM usage: {vram_mb:.2f} MB\")\n",
        "    print(f\"Entity '{entity}' preserved: {'âœ“ YES' if entity_preserved else 'âœ— NO'}\")\n",
        "    \n",
        "    # Show which tokens are kept\n",
        "    kept_indices = np.where(mask)[0]\n",
        "    kept_token_strs = [tokens[i] for i in kept_indices]\n",
        "    print(f\"\\nKept token positions: {kept_indices.tolist()}\")\n",
        "    print(f\"Kept tokens: {kept_token_strs}\")\n",
        "    \n",
        "    # Highlight if entity position is in kept set\n",
        "    if entity_token_idx is not None:\n",
        "        if entity_preserved:\n",
        "            print(f\"âœ“ Entity token '{entity_token}' at position {entity_token_idx} is PRESERVED\")\n",
        "        else:\n",
        "            print(f\"âœ— Entity token '{entity_token}' at position {entity_token_idx} was EVICTED\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Simulation complete!\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results: Entity Preservation by Policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bar chart with color coding\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Extract data\n",
        "policy_names = [r['policy'] for r in results]\n",
        "vram_usage = [r['vram_mb'] for r in results]\n",
        "entity_preserved = [r['entity_preserved'] for r in results]\n",
        "kept_tokens = [r['kept_tokens'] for r in results]\n",
        "\n",
        "# Color code: Green if entity preserved, Red if evicted\n",
        "colors = ['green' if preserved else 'red' for preserved in entity_preserved]\n",
        "\n",
        "# Plot 1: VRAM Usage\n",
        "bars1 = ax1.bar(policy_names, vram_usage, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax1.set_ylabel('VRAM Usage (MB)', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Eviction Policy', fontsize=12, fontweight='bold')\n",
        "ax1.set_title(f'VRAM Usage by Policy\\nGreen = Entity \"{entity}\" Preserved | Red = Entity Evicted', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, vram, preserved in zip(bars1, vram_usage, entity_preserved):\n",
        "    height = bar.get_height()\n",
        "    symbol = 'âœ“' if preserved else 'âœ—'\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{vram:.1f} MB\\n{symbol}',\n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# Plot 2: Tokens Kept\n",
        "bars2 = ax2.bar(policy_names, kept_tokens, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax2.set_ylabel('Tokens Kept', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Eviction Policy', fontsize=12, fontweight='bold')\n",
        "ax2.set_title(f'Tokens Retained by Policy (Total: {len(tokens)})\\nGreen = Entity \"{entity}\" Preserved | Red = Entity Evicted', \n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "ax2.axhline(y=len(tokens), color='blue', linestyle='--', alpha=0.5, label='Total tokens')\n",
        "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "ax2.legend()\n",
        "\n",
        "# Add value labels\n",
        "for bar, kept, preserved in zip(bars2, kept_tokens, entity_preserved):\n",
        "    height = bar.get_height()\n",
        "    symbol = 'âœ“' if preserved else 'âœ—'\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{kept}\\n{symbol}',\n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"{'Policy':<20} {'VRAM (MB)':<15} {'Tokens Kept':<15} {'Entity Preserved':<20}\")\n",
        "print(\"=\"*80)\n",
        "for r in results:\n",
        "    preserved_str = 'âœ“ YES' if r['entity_preserved'] else 'âœ— NO'\n",
        "    print(f\"{r['policy']:<20} {r['vram_mb']:<15.2f} {r['kept_tokens']:<15} {preserved_str:<20}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Token Retention Patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a visual representation of which tokens are kept\n",
        "fig, axes = plt.subplots(len(results), 1, figsize=(16, 3*len(results)))\n",
        "\n",
        "if len(results) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, result in zip(axes, results):\n",
        "    mask = result['mask']\n",
        "    policy = result['policy']\n",
        "    entity_preserved = result['entity_preserved']\n",
        "    \n",
        "    # Create color array: green for kept, red for evicted\n",
        "    colors = ['lightgreen' if m else 'lightcoral' for m in mask]\n",
        "    \n",
        "    # Highlight entity token\n",
        "    if entity_token_idx is not None:\n",
        "        colors[entity_token_idx] = 'gold' if entity_preserved else 'darkred'\n",
        "    \n",
        "    # Create bar chart\n",
        "    positions = range(len(tokens))\n",
        "    ax.bar(positions, np.ones(len(tokens)), color=colors, edgecolor='black', linewidth=0.5)\n",
        "    \n",
        "    # Set labels\n",
        "    ax.set_xticks(positions)\n",
        "    ax.set_xticklabels(tokens, rotation=90, ha='right', fontsize=8)\n",
        "    ax.set_ylabel('Retained', fontsize=10)\n",
        "    ax.set_ylim(0, 1.2)\n",
        "    ax.set_yticks([0, 1])\n",
        "    ax.set_yticklabels(['Evicted', 'Kept'])\n",
        "    \n",
        "    # Title with entity preservation status\n",
        "    status = 'âœ“ PRESERVED' if entity_preserved else 'âœ— EVICTED'\n",
        "    title_color = 'green' if entity_preserved else 'red'\n",
        "    ax.set_title(f\"{policy}: Entity '{entity}' {status}\", \n",
        "                fontsize=12, fontweight='bold', color=title_color, pad=10)\n",
        "    \n",
        "    # Add vertical line at entity position\n",
        "    if entity_token_idx is not None:\n",
        "        ax.axvline(x=entity_token_idx, color='orange', linestyle='--', linewidth=2, alpha=0.7)\n",
        "        ax.text(entity_token_idx, 1.1, f'â† {entity}', ha='center', fontweight='bold', fontsize=10)\n",
        "    \n",
        "    ax.grid(axis='x', alpha=0.2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insights\n",
        "\n",
        "From the \"Needle in a Haystack\" experiment:\n",
        "\n",
        "1. **Full Cache**: Always preserves all tokens (baseline)\n",
        "2. **Sliding Window**: May evict important tokens from the beginning\n",
        "3. **StreamingLLM**: Preserves attention sinks (first tokens) + recent window\n",
        "4. **H2O**: Preserves tokens based on attention scores (heavy hitters)\n",
        "\n",
        "**Critical Finding**: \n",
        "- If your entity \"Alice\" appears early, **StreamingLLM** will preserve it (via sinks)\n",
        "- If \"Alice\" has high attention, **H2O** will preserve it (via heavy hitter detection)\n",
        "- **Sliding Window** may lose it entirely if it's not in the recent window\n",
        "\n",
        "This demonstrates why **attention-aware policies** (StreamingLLM, H2O) outperform naive approaches for maintaining context coherence!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "âœ… **Loading GPT-2** and capturing real attention patterns  \n",
        "âœ… **Using SinkVis** context manager for instrumentation  \n",
        "âœ… **Visualizing attention sinks** with heatmaps  \n",
        "âœ… **Comparing eviction policies** (Full, Sliding Window, StreamingLLM, H2O)  \n",
        "âœ… **Needle in a Haystack test** - checking entity preservation  \n",
        "âœ… **VRAM calculation** for each policy  \n",
        "âœ… **Color-coded visualization** showing preservation success  \n",
        "\n",
        "**Next Steps:**\n",
        "- Try different models (e.g., `gpt2-medium`, `distilgpt2`)\n",
        "- Experiment with longer sequences\n",
        "- Adjust cache budgets and sink counts\n",
        "- Test with domain-specific prompts\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

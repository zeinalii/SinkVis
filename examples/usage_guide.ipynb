{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SinkVis Usage Guide\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zeinalii/SinkVis/blob/main/examples/usage_guide.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use SinkVis in your projects for:\n",
        "1. Analyzing attention patterns\n",
        "2. Testing eviction policies\n",
        "3. Integrating with HuggingFace models\n",
        "4. Live streaming to the visualizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, clone SinkVis and install dependencies (required for Google Colab):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SinkVis path: /Users/amir/Documents/stuff/SinkVis\n"
          ]
        }
      ],
      "source": [
        "# Clone SinkVis repository (for Google Colab)\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "if not os.path.exists('SinkVis'):\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/zeinalii/SinkVis.git'], check=True)\n",
        "    print(\"✓ Repository cloned\")\n",
        "else:\n",
        "    print(\"✓ Repository already exists\")\n",
        "\n",
        "# Install dependencies\n",
        "%pip install -q matplotlib numpy websockets\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add SinkVis to path (works for both Colab and local)\n",
        "SINKVIS_PATH = Path(\"SinkVis\").resolve() if Path(\"SinkVis\").exists() else Path(\".\").resolve().parent\n",
        "sys.path.insert(0, str(SINKVIS_PATH))\n",
        "\n",
        "print(f\"✓ SinkVis loaded from: {SINKVIS_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Identify Attention Sinks and Heavy Hitters\n",
        "\n",
        "Use SinkVis to find important tokens in any attention matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention matrix shape: (32, 32)\n",
            "Each row sums to: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from backend.attention import (\n",
        "    generate_attention_pattern,\n",
        "    identify_sinks,\n",
        "    identify_heavy_hitters,\n",
        "    create_attention_frame\n",
        ")\n",
        "\n",
        "# Generate a realistic attention pattern (or use your own)\n",
        "seq_len = 32\n",
        "attention = generate_attention_pattern(seq_len, num_sinks=4)\n",
        "\n",
        "print(f\"Attention matrix shape: {attention.shape}\")\n",
        "print(f\"Each row sums to: {attention[10, :11].sum():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention sinks at positions: []\n",
            "Heavy hitters at positions: [0, 1, 2, 3, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n"
          ]
        }
      ],
      "source": [
        "# Find attention sinks (tokens that receive high attention)\n",
        "sinks = identify_sinks(attention, threshold=0.1)\n",
        "print(f\"Attention sinks at positions: {sinks}\")\n",
        "\n",
        "# Find heavy hitters (semantically important tokens)\n",
        "heavy_hitters = identify_heavy_hitters(attention, threshold=0.05, exclude_sinks=sinks)\n",
        "print(f\"Heavy hitters at positions: {heavy_hitters}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize with matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "im = ax.imshow(attention, cmap='hot', aspect='auto')\n",
        "\n",
        "# Mark sinks with cyan lines\n",
        "for s in sinks:\n",
        "    ax.axvline(x=s, color='cyan', linewidth=2, label='Sink' if s == sinks[0] else '')\n",
        "\n",
        "# Mark heavy hitters with green lines\n",
        "for h in heavy_hitters:\n",
        "    ax.axvline(x=h, color='lime', linewidth=1, linestyle='--', label='Heavy Hitter' if h == heavy_hitters[0] else '')\n",
        "\n",
        "ax.set_xlabel('Key Position')\n",
        "ax.set_ylabel('Query Position')\n",
        "ax.set_title('Attention Pattern with Sinks and Heavy Hitters')\n",
        "plt.colorbar(im, ax=ax, label='Attention Weight')\n",
        "ax.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Test Eviction Policies\n",
        "\n",
        "Compare different KV cache eviction strategies on your prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from backend.eviction import run_simulation, tokenize_simple\n",
        "from backend.models import SimulationConfig, EvictionPolicy\n",
        "\n",
        "# Your conversation/prompt\n",
        "prompt = \"\"\"\n",
        "System: You are a helpful assistant. The user's name is Alice and she likes cats.\n",
        "User: What's my name?\n",
        "Assistant: Your name is Alice.\n",
        "User: Tell me a long story about dragons and knights.\n",
        "Assistant: Once upon a time in a kingdom far away, there lived a brave knight...\n",
        "User: What pet do I like?\n",
        "\"\"\"\n",
        "\n",
        "# See how it tokenizes\n",
        "tokens = tokenize_simple(prompt)\n",
        "print(f\"Token count: {len(tokens)}\")\n",
        "print(f\"First 10 tokens: {tokens[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test StreamingLLM policy\n",
        "config = SimulationConfig(\n",
        "    policy=EvictionPolicy.STREAMING_LLM,\n",
        "    cache_size=64,       # Max tokens in cache\n",
        "    sink_count=4,        # Preserve first 4 tokens\n",
        "    window_size=32       # Keep last 32 tokens\n",
        ")\n",
        "\n",
        "result = run_simulation(prompt, config, generate_tokens=32)\n",
        "\n",
        "print(f\"Policy: {result.policy.value}\")\n",
        "print(f\"Tokens processed: {result.total_tokens_processed}\")\n",
        "print(f\"Cache hits: {result.cache_hits}\")\n",
        "print(f\"Cache misses: {result.cache_misses}\")\n",
        "print(f\"Evictions: {result.evictions}\")\n",
        "print(f\"Retained sinks: {result.retained_sinks}\")\n",
        "print(f\"Retained heavy hitters: {result.retained_heavy_hitters}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare ALL eviction policies\n",
        "policies = [\n",
        "    EvictionPolicy.FULL,           # No eviction (baseline)\n",
        "    EvictionPolicy.LRU,            # Least Recently Used\n",
        "    EvictionPolicy.SLIDING_WINDOW, # Keep only recent tokens\n",
        "    EvictionPolicy.STREAMING_LLM,  # Sinks + sliding window\n",
        "    EvictionPolicy.H2O,            # Sinks + heavy hitters\n",
        "]\n",
        "\n",
        "results = []\n",
        "for policy in policies:\n",
        "    config = SimulationConfig(\n",
        "        policy=policy,\n",
        "        cache_size=64,\n",
        "        sink_count=4,\n",
        "        window_size=32\n",
        "    )\n",
        "    result = run_simulation(prompt, config, generate_tokens=32)\n",
        "    results.append({\n",
        "        'policy': policy.value,\n",
        "        'evictions': result.evictions,\n",
        "        'sinks_kept': result.retained_sinks,\n",
        "        'heavy_kept': result.retained_heavy_hitters,\n",
        "        'cached': result.final_cache.total_tokens\n",
        "    })\n",
        "\n",
        "# Display results\n",
        "for r in results:\n",
        "    print(f\"{r['policy']:15} | evictions: {r['evictions']:3} | sinks: {r['sinks_kept']} | cached: {r['cached']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "policies_names = [r['policy'] for r in results]\n",
        "\n",
        "axes[0].bar(policies_names, [r['evictions'] for r in results], color='coral')\n",
        "axes[0].set_title('Evictions (lower is better)')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[1].bar(policies_names, [r['sinks_kept'] for r in results], color='cyan')\n",
        "axes[1].set_title('Sinks Retained')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[2].bar(policies_names, [r['cached'] for r in results], color='lime')\n",
        "axes[2].set_title('Final Cache Size')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Integrate with HuggingFace Transformers\n",
        "\n",
        "Extract real attention weights from a HuggingFace model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install if needed: pip install transformers torch\n",
        "try:\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    import torch\n",
        "    HF_AVAILABLE = True\n",
        "    print(\"✓ HuggingFace transformers available\")\n",
        "except ImportError:\n",
        "    print(\"✗ Install with: pip install transformers torch\")\n",
        "    HF_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if HF_AVAILABLE:\n",
        "    # Load a small model (GPT-2)\n",
        "    model_name = \"gpt2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Loaded {model_name}\")\n",
        "    print(f\"Layers: {model.config.n_layer}, Heads: {model.config.n_head}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if HF_AVAILABLE:\n",
        "    # Run inference and get attention\n",
        "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Get attention from last layer, first head\n",
        "    # Shape: (batch, heads, seq_len, seq_len)\n",
        "    attention_weights = outputs.attentions[-1][0, 0].numpy()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    \n",
        "    print(f\"Attention shape: {attention_weights.shape}\")\n",
        "    print(f\"Tokens: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if HF_AVAILABLE:\n",
        "    # Analyze with SinkVis\n",
        "    sinks = identify_sinks(attention_weights, threshold=0.15)\n",
        "    heavy = identify_heavy_hitters(attention_weights, threshold=0.08, exclude_sinks=sinks)\n",
        "    \n",
        "    print(f\"\\nAttention Sinks: {[tokens[i] for i in sinks]}\")\n",
        "    print(f\"Heavy Hitters: {[tokens[i] for i in heavy]}\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    im = ax.imshow(attention_weights, cmap='hot')\n",
        "    ax.set_xticks(range(len(tokens)))\n",
        "    ax.set_yticks(range(len(tokens)))\n",
        "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
        "    ax.set_yticklabels(tokens)\n",
        "    ax.set_xlabel('Key (attended to)')\n",
        "    ax.set_ylabel('Query (attending from)')\n",
        "    ax.set_title(f'GPT-2 Attention (Last Layer, Head 0)')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Stream to Live Visualizer\n",
        "\n",
        "Send attention data to the SinkVis web interface in real-time.\n",
        "\n",
        "**First, start the server in a terminal:**\n",
        "```bash\n",
        "cd SinkVis && python run.py\n",
        "```\n",
        "\n",
        "Then open http://localhost:8765\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import websockets\n",
        "    WS_AVAILABLE = True\n",
        "    print(\"✓ websockets available\")\n",
        "except ImportError:\n",
        "    print(\"✗ Install with: pip install websockets\")\n",
        "    WS_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def send_attention_to_sinkvis(attention_matrix, tokens, layer=0, head=0):\n",
        "    \"\"\"\n",
        "    Send an attention matrix to SinkVis for visualization.\n",
        "    \n",
        "    Args:\n",
        "        attention_matrix: numpy array of shape (seq_len, seq_len)\n",
        "        tokens: list of token strings\n",
        "        layer: layer number (for display)\n",
        "        head: head number (for display)\n",
        "    \"\"\"\n",
        "    uri = \"ws://localhost:8765/ws/attention\"\n",
        "    \n",
        "    # Find sinks and heavy hitters\n",
        "    sinks = identify_sinks(attention_matrix, threshold=0.1)\n",
        "    heavy = identify_heavy_hitters(attention_matrix, threshold=0.05, exclude_sinks=sinks)\n",
        "    \n",
        "    frame = {\n",
        "        \"type\": \"frame\",\n",
        "        \"data\": {\n",
        "            \"layer\": layer,\n",
        "            \"head\": head,\n",
        "            \"seq_len\": len(tokens),\n",
        "            \"attention_weights\": attention_matrix.tolist(),\n",
        "            \"token_labels\": tokens,\n",
        "            \"sink_indices\": sinks,\n",
        "            \"heavy_hitter_indices\": heavy,\n",
        "            \"timestamp\": 0.0\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        async with websockets.connect(uri) as ws:\n",
        "            await ws.send(json.dumps(frame))\n",
        "            print(f\"✓ Sent attention frame ({len(tokens)} tokens)\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Could not connect to SinkVis: {e}\")\n",
        "        print(\"  Make sure the server is running: python run.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send synthetic attention to visualizer\n",
        "if WS_AVAILABLE:\n",
        "    test_attention = generate_attention_pattern(20)\n",
        "    test_tokens = [f\"tok_{i}\" for i in range(20)]\n",
        "    \n",
        "    # Run async function\n",
        "    await send_attention_to_sinkvis(test_attention, test_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "---\n",
        "## 5. Cache Block Analysis\n",
        "\n",
        "Analyze how tokens are distributed across memory tiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from backend.attention import generate_cache_blocks\n",
        "from backend.models import MemoryTier\n",
        "\n",
        "# Generate cache blocks for a 256-token sequence\n",
        "blocks = generate_cache_blocks(\n",
        "    seq_len=256,\n",
        "    block_size=16,\n",
        "    sink_indices=[0, 1, 2, 3],\n",
        "    heavy_hitter_indices=[50, 100, 150]\n",
        ")\n",
        "\n",
        "print(f\"Total blocks: {len(blocks)}\")\n",
        "\n",
        "# Count by tier\n",
        "tier_counts = {}\n",
        "tier_sizes = {}\n",
        "for block in blocks:\n",
        "    tier = block.memory_tier.value\n",
        "    tier_counts[tier] = tier_counts.get(tier, 0) + 1\n",
        "    tier_sizes[tier] = tier_sizes.get(tier, 0) + block.size_bytes\n",
        "\n",
        "print(\"\\nBlocks per tier:\")\n",
        "for tier, count in tier_counts.items():\n",
        "    print(f\"  {tier}: {count} blocks ({tier_sizes[tier] / 1024:.1f} KB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize memory hierarchy\n",
        "tier_colors = {\n",
        "    'gpu_hbm': '#00ff88',\n",
        "    'gpu_l2': '#00ccff',\n",
        "    'system_ram': '#ffd93d',\n",
        "    'disk': '#ff6b9d'\n",
        "}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 3))\n",
        "\n",
        "for i, block in enumerate(blocks):\n",
        "    color = tier_colors[block.memory_tier.value]\n",
        "    edgecolor = 'red' if block.is_sink else ('cyan' if block.is_heavy_hitter else 'none')\n",
        "    linewidth = 2 if block.is_sink or block.is_heavy_hitter else 0\n",
        "    \n",
        "    ax.bar(i, 1, color=color, edgecolor=edgecolor, linewidth=linewidth)\n",
        "\n",
        "ax.set_xlabel('Block Index')\n",
        "ax.set_title('KV Cache Memory Hierarchy')\n",
        "ax.set_yticks([])\n",
        "\n",
        "# Legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#00ff88', label='GPU HBM'),\n",
        "    Patch(facecolor='#00ccff', label='GPU L2'),\n",
        "    Patch(facecolor='#ffd93d', label='System RAM'),\n",
        "    Patch(facecolor='#ff6b9d', label='Disk'),\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "| Function | Use Case |\n",
        "|----------|----------|\n",
        "| `identify_sinks()` | Find tokens receiving high attention |\n",
        "| `identify_heavy_hitters()` | Find semantically important tokens |\n",
        "| `run_simulation()` | Test eviction policies on prompts |\n",
        "| `generate_cache_blocks()` | Analyze memory hierarchy |\n",
        "| WebSocket streaming | Real-time visualization |\n",
        "\n",
        "For more, see the [README.md](../README.md) or run the web interface:\n",
        "```bash\n",
        "python run.py\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

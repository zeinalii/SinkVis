{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SinkVis Demo with Real LLM\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zeinalii/SinkVis/blob/main/examples/real_model_demo.ipynb)\n",
        "\n",
        "This notebook demonstrates SinkVis with a real open-source language model.\n",
        "\n",
        "We'll:\n",
        "1. **Setup**: Clone SinkVis and install dependencies\n",
        "2. Download DistilGPT-2 (small, fast, ~350MB)\n",
        "3. Extract real attention patterns\n",
        "4. Identify attention sinks and heavy hitters\n",
        "5. Simulate KV cache eviction policies\n",
        "6. Visualize results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Clone SinkVis & Install Dependencies\n",
        "\n",
        "Run this cell first to set up the environment (required for Google Colab).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone SinkVis repository\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "if not os.path.exists('SinkVis'):\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/zeinalii/SinkVis.git'], check=True)\n",
        "    print(\"✓ Repository cloned\")\n",
        "else:\n",
        "    print(\"✓ Repository already exists\")\n",
        "\n",
        "# Install dependencies\n",
        "%pip install -q transformers torch matplotlib numpy websockets\n",
        "\n",
        "print(\"✓ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add SinkVis to path (works for both Colab and local)\n",
        "SINKVIS_PATH = Path(\"SinkVis\").resolve() if Path(\"SinkVis\").exists() else Path(\".\").resolve().parent\n",
        "sys.path.insert(0, str(SINKVIS_PATH))\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import SinkVis modules\n",
        "from backend.attention import identify_sinks, identify_heavy_hitters\n",
        "from backend.eviction import run_simulation\n",
        "from backend.models import SimulationConfig, EvictionPolicy\n",
        "\n",
        "print(f\"✓ SinkVis loaded from: {SINKVIS_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Download and Load Model\n",
        "\n",
        "We'll use **DistilGPT-2** - a distilled version of GPT-2 that's:\n",
        "- Small (~350MB)\n",
        "- Fast to run on CPU\n",
        "- Has 6 layers, 12 attention heads\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Download and load model (first run will download ~350MB)\n",
        "MODEL_NAME = \"distilgpt2\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, output_attentions=True)\n",
        "model.eval()\n",
        "\n",
        "print(f\"✓ Model loaded!\")\n",
        "print(f\"  - Layers: {model.config.n_layer}\")\n",
        "print(f\"  - Heads per layer: {model.config.n_head}\")\n",
        "print(f\"  - Hidden size: {model.config.n_embd}\")\n",
        "print(f\"  - Vocab size: {model.config.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run Inference and Extract Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input prompt - try changing this!\n",
        "prompt = \"\"\"The transformer architecture has revolutionized natural language processing. \n",
        "Self-attention allows models to focus on relevant parts of the input sequence. \n",
        "The key insight is that attention weights determine which tokens influence the output.\"\"\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "print(f\"Input: {len(tokens)} tokens\")\n",
        "print(f\"Tokens: {tokens[:10]}... (showing first 10)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run model and get attention weights\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# outputs.attentions is a tuple of (batch, heads, seq, seq) for each layer\n",
        "print(f\"\\nAttention tensors: {len(outputs.attentions)} layers\")\n",
        "print(f\"Shape per layer: {outputs.attentions[0].shape}\")\n",
        "\n",
        "# Extract attention from last layer, average across heads\n",
        "last_layer_attention = outputs.attentions[-1][0]  # (heads, seq, seq)\n",
        "avg_attention = last_layer_attention.mean(dim=0).numpy()  # (seq, seq)\n",
        "\n",
        "print(f\"\\nAveraged attention shape: {avg_attention.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Identify Attention Sinks\n",
        "\n",
        "**Attention sinks** are tokens that receive disproportionately high attention regardless of their semantic content. Research shows these are typically:\n",
        "- The BOS (beginning of sequence) token\n",
        "- First few tokens in the sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find attention sinks\n",
        "sinks = identify_sinks(avg_attention, threshold=0.08)\n",
        "print(\"Attention Sinks:\")\n",
        "for idx in sinks:\n",
        "    avg_weight = avg_attention[:, idx].mean()\n",
        "    print(f\"  Position {idx}: '{tokens[idx]}' (avg attention: {avg_weight:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find heavy hitters (semantically important tokens)\n",
        "heavy_hitters = identify_heavy_hitters(avg_attention, threshold=0.03, exclude_sinks=sinks)\n",
        "print(\"\\nHeavy Hitters (semantically important):\")\n",
        "for idx in heavy_hitters[:10]:  # Show top 10\n",
        "    avg_weight = avg_attention[:, idx].mean()\n",
        "    print(f\"  Position {idx}: '{tokens[idx]}' (avg attention: {avg_weight:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
